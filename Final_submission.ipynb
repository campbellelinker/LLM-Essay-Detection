{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ded7f3",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Names: Sophia Hillard & Campbell Linker\n",
    "\n",
    "- Insert Markdown chunks for your written responses as needed\n",
    "- Do not include unnecessary code. Only code that is needed to answer the questions should be included, nothing more. \n",
    "- Be sure your work is **reproducible** by \"Restart and Clear Output\" then \"Run All\" cells\n",
    "- The group leader will make a single submission on Moodle on behalf of the group. \n",
    "- **ACKNOWELDGE ALL EXTERNAL SOURCES HERE:** Give a brief summary of any external sources you used. \n",
    "    - See works cited in our write up for sources we used for **conceptual background** (e.g., TF-IDF). Below, we are including the sources we used to support our **coding process.**\n",
    "        - Sophia: [EDA for feature engineering](https://pankaj8blr.medium.com/eda-and-significance-of-various-plots-in-feature-engineering-f8cededbd520)\n",
    "        - Sophia: [EDA Seaborn Boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html)\n",
    "        - Cam: [**PySpellChecker**](https://pypi.org/project/pyspellchecker/) was used in our original feature engineering, but we abandoned the idea in our final model as the numeric values were not calculated with appropriate sensitivity. We struggled with over/under detection of true spelling errors. However, when we were using this as a feature engineering step, we followed the previously linked website and [this site](https://www.geeksforgeeks.org/python/spelling-checker-in-python/) for guidance with building the package into a function.\n",
    "        - Cam: We made frequent references to the [**discussion page**](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion?sort=hotness) of the Kaggle competition. Nothing on this page informed our code, but we did use it for reccomendations in using TF-IDF and identifying data. Multiple threads inspired our decisions but none influenced code.\n",
    "        - Cam: I referenced **Google AI Overview** for how to sequence out our preprocessing pipeline, once we incorporated TF-IDF. There were a few key conceptual and code takeaways from this search, I found it to be a helpful synthesis of information appearing across multiple sites:\n",
    "            - The scaling step (we used MaxAbsScaler) should NOT be applied to the TF-IDF columns. This is why we divided our preproccessing pipeline into two sections.\n",
    "            - The first section handles our NLP step. Google AI Overview suggested pseudocode for how to combine the text and character vectorizer steps into one, such that they can be used in the preprocessing pipeline. [FeatureUnion()](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) was suggested, and we referenced the function documentation and [this page](https://www.kaggle.com/discussions/questions-and-answers/55544) for how to use it when combining NLP text features. The NLP step functions as a transformer to the original text data in this preprocessing pipeline.\n",
    "            - The next section handles our human-feature engineering. It takes `our_features` and scales them. We knew how to do this step from class but Google AI Overview provided direction in how to combine this step with new information regarding NLP.\n",
    "        - Cam: I did a lot of research about parameters for each of our models. We struggled with a lot of overfitting and I wasn't sure which parameters would be best at addressing this issue. Google AI Overview came up with each search but did not produce anything helpful. These were the **websites** I found most helpful in this process:\n",
    "            - [XGBoost Parameter tuning - \"Control Overfitting\"](https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html)\n",
    "            - [Random Forrest](https://www.geeksforgeeks.org/machine-learning/random-forest-hyperparameter-tuning-in-python/) -- We abandoned this model pretty early on\n",
    "            - [Logisitc Regression Parameter tuning](https://stackoverflow.com/questions/21816346/fine-tuning-parameters-in-logistic-regression) - helpful for a review from class, though there was not much to experiment with.\n",
    "\n",
    "**Please note** that our EDA step was moved to come after feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b0ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import math\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings   \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcddc3a9",
   "metadata": {},
   "source": [
    "## 1. The Data\n",
    "\n",
    "The dataset we chose is [LLM - Detect AI Generated Text](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/overview) where:\n",
    "\n",
    "* Outcome variable: Binary, AI generated (1) or human generated (0)\n",
    "* Predictor variables: -- all created through feature engineering of essay text\n",
    "* Number of observations:\n",
    "    * Training set: $n_{train} = 9000$ ($1000$ from competition training set, $8000$ from [reccomended external source](https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset))\n",
    "    * Test set: $n_{test} = 9000$\n",
    "* Score/metric used for the [leaderboard](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/leaderboard): Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9575fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all datasets here. \n",
    "train = pd.read_csv(\"data/all-DAIGT-training.csv\", sep=',', index_col = 'id') # see write-up for data provenance\n",
    "test_essays = pd.read_csv('data/test_essays.csv', index_col='id')\n",
    "example = pd.read_csv('data/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e56f37",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Perform your feature engineering here and then create `y_train`, `X_train` and `X_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94ce9338-08fa-457a-9d9d-62a83b383783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for variables \n",
    "\n",
    "input_transition = [\"first\", \"firstly\", \"second\", \"secondly\", \"third\", \"thirdly\", \"meanwhile\", \"previously\", \"subsequently\", \"eventually\", \n",
    "                    \"finally\", \"lastly\", \"ultimately\", \"conclusion\", \"addition\", \"additionally\", \"furthermore\", \"moreover\", \"besides\", \n",
    "                    \"equally\", \"however\", \"contrary\", \"conversely\", \"despite\", \"contrast\", \"nevertheless\", \"nonetheless\", \"whereas\", \"while\",\n",
    "                    \"although\", \"though\", \"therefore\", \"thus\", \"hence\", \"consequently\", \"accordingly\", \"namely\", \"specifically\", \"indeed\", \n",
    "                    \"importantly\", \"significantly\", \"especially\", \"notably\", \"undoubtedly\", \"likewise\", \"similarly\", \"correspondingly\", \"sum\",\n",
    "                    \"summary\", \"overall\", \"conclude\", \"conclusion\", \"simultaneously\", \"formerly\", \"lately\", \"recently\", \"opposite\", \"adjacent\",\n",
    "                    \"provided\", \"admittedly\", \"regarding\"]\n",
    "\n",
    "input_hyperbole = [\"powerful\", \"groundbreaking\", \"illuminating\", \"vital\", \"invaluable\", \"indelible\", \"essential\", \"poignant\", \"profound\", \n",
    "                   \"remarkable\", \"transformative\", \"revolutionary\", \"unparalleled\", \"extraordinary\", \"compelling\", \"significant\", \"exceptional\",\n",
    "                   \"crucial\", \"monumental\", \"dramatic\", \"robust\", \"innovative\", \"pivotal\", \"impressive\", \"astonishing\", \"visionary\", \"inspiring\",\n",
    "                   \"striking\", \"dynamic\", \"iconic\", \"seminal\", \"trailblazing\", \"revolutionary\", \"extreme\", \"shocking\"]\n",
    "\n",
    "input_abn_symbols = [\"[\", \"]\", \"_\", \"*\", \"<\", \">\", \"{\", \"}\", \"^\", \"@\", \"#\", \"|\", \"\\\\\"]\n",
    "\n",
    "input_prompt_lang = [\"here you go\", \"as an ai\", \"as a language model\", \"i generated\", \"here's the essay\", \"here's your essay\", \"let me\", \n",
    "                     \"help you\", \"sure,\", \"i hope this helps\", \"your prompt\", \"your request\", \"here is\", \"here's\", \"sure!\", \n",
    "                     \"here is the essay\", \"here is your essay\", \"language model\", \"large language\", \"llm\", \"generative ai\", \"chatbot\", \n",
    "                     \"your essay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f70da207-5f9b-45d6-849f-4d25b55cebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions for feature engineering \n",
    "\n",
    "### Spelling errors\n",
    "spell = SpellChecker()\n",
    "\n",
    "def clean_text_for_spellcheck(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    for symbol in input_abn_symbols:\n",
    "        text = text.replace(symbol, \"\")\n",
    "    text = re.sub(r\"[^a-z\\s'-]\", \"\", text.lower())\n",
    "    return text\n",
    "\n",
    "def misspelling_ratio(text):\n",
    "    text_clean = clean_text_for_spellcheck(text)\n",
    "    words = text_clean.split()\n",
    "    if len(words) == 0:\n",
    "        return 0.0 \n",
    "    misspelled = spell.unknown(words)\n",
    "    return len(misspelled) / len(words)\n",
    "\n",
    "def count_misspellings(text):\n",
    "    text_clean = clean_text_for_spellcheck(text)\n",
    "    words = text_clean.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    misspelled = spell.unknown(words)\n",
    "    return len(misspelled)\n",
    "\n",
    "### Exclamation points\n",
    "def count_exclamation_points(text):\n",
    "    return text.count('!')\n",
    "\n",
    "\n",
    "### Em dashes\n",
    "def count_em_dash(text):\n",
    "    return text.count('â€”')\n",
    "\n",
    "\n",
    "### Transitional words\n",
    "def count_transition(text):\n",
    "    return sum(text.count(word) for word in input_transition)\n",
    "\n",
    "\n",
    "### Hyperbolic phrasing\n",
    "def count_hyperbolic(text):\n",
    "    return sum(text.count(word) for word in input_hyperbole)\n",
    "\n",
    "\n",
    "### Abnormal symbols\n",
    "def count_abn_symbols(text):\n",
    "    return sum(text.count(word) for word in input_abn_symbols)\n",
    "\n",
    "\n",
    "## Prompt indicator\n",
    "def contains_prompt_indicators(text):\n",
    "    return sum(text.count(word) for word in input_prompt_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cca0a5f-5ff5-4353-b4e1-7cc4dcdb9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features to both training AND competition test data\n",
    "\n",
    "def add_features(df):\n",
    "    # making sure to include a step that makes all text lowercase so that features can be applied\n",
    "    df[\"text\"] = df[\"text\"].str.lower()  \n",
    "    #df[\"misspelling_ratio\"] = df[\"text\"].apply(misspelling_ratio)\n",
    "    #df[\"n_misspellings\"] = df[\"text\"].apply(count_misspellings)\n",
    "    df[\"n_exclamations\"] = df[\"text\"].apply(count_exclamation_points)\n",
    "    df[\"n_em_dash\"] = df[\"text\"].apply(count_em_dash)\n",
    "    df[\"n_transition\"] = df[\"text\"].apply(count_transition)\n",
    "    df[\"n_hyperbolic\"] = df[\"text\"].apply(count_hyperbolic)\n",
    "    df[\"n_abn_symbols\"] = df[\"text\"].apply(count_abn_symbols)\n",
    "    df[\"n_prompt\"] = df[\"text\"].apply(contains_prompt_indicators)\n",
    "\n",
    "    return df\n",
    "\n",
    "train = add_features(train)\n",
    "test_essays = add_features(test_essays)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cacf85d-f4cb-40f5-9802-775dded20d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>model</th>\n",
       "      <th>n_exclamations</th>\n",
       "      <th>n_em_dash</th>\n",
       "      <th>n_transition</th>\n",
       "      <th>n_hyperbolic</th>\n",
       "      <th>n_abn_symbols</th>\n",
       "      <th>n_prompt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d429f032</th>\n",
       "      <td>advantages of limiting car usage \\n\\nlimiting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1ce279be</th>\n",
       "      <td>advantages of limiting car usage\\n\\nlimiting c...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c9595213</th>\n",
       "      <td>limiting car usage has numerous advantages tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f2266d87</th>\n",
       "      <td>the passages provided discuss the advantages o...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eeace4bd</th>\n",
       "      <td>title: the advantages of limiting car usage\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354fdce0</th>\n",
       "      <td>advantages of limiting car usage\\n\\nlimiting c...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6eaa842f</th>\n",
       "      <td>the advantages of limiting car usage are becom...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a36a04d5</th>\n",
       "      <td>limiting car usage has numerous advantages for...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c9d5567f</th>\n",
       "      <td>advantages of limiting car usage\\n\\nlimiting c...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2e2ead1</th>\n",
       "      <td>the advantages of limiting car usage\\n\\nin rec...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9408e5f4</th>\n",
       "      <td>it is becoming increasingly evident that there...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ee18417a</th>\n",
       "      <td>explanatory essay: the advantages of limiting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35f1f42a</th>\n",
       "      <td>title: the advantages of limiting car usage\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55652348</th>\n",
       "      <td>limiting car usage offers numerous advantages ...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72a3909f</th>\n",
       "      <td>advantages of limiting car usage\\n\\nin recent ...</td>\n",
       "      <td>1</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text  generated  \\\n",
       "id                                                                       \n",
       "d429f032  advantages of limiting car usage \\n\\nlimiting ...          1   \n",
       "1ce279be  advantages of limiting car usage\\n\\nlimiting c...          1   \n",
       "c9595213  limiting car usage has numerous advantages tha...          1   \n",
       "f2266d87  the passages provided discuss the advantages o...          1   \n",
       "eeace4bd  title: the advantages of limiting car usage\\n\\...          1   \n",
       "354fdce0  advantages of limiting car usage\\n\\nlimiting c...          1   \n",
       "6eaa842f  the advantages of limiting car usage are becom...          1   \n",
       "a36a04d5  limiting car usage has numerous advantages for...          1   \n",
       "c9d5567f  advantages of limiting car usage\\n\\nlimiting c...          1   \n",
       "c2e2ead1  the advantages of limiting car usage\\n\\nin rec...          1   \n",
       "9408e5f4  it is becoming increasingly evident that there...          1   \n",
       "ee18417a  explanatory essay: the advantages of limiting ...          1   \n",
       "35f1f42a  title: the advantages of limiting car usage\\n\\...          1   \n",
       "55652348  limiting car usage offers numerous advantages ...          1   \n",
       "72a3909f  advantages of limiting car usage\\n\\nin recent ...          1   \n",
       "\n",
       "                  model  n_exclamations  n_em_dash  n_transition  \\\n",
       "id                                                                 \n",
       "d429f032  gpt-3.5-turbo               0          0             7   \n",
       "1ce279be  gpt-3.5-turbo               0          0             6   \n",
       "c9595213  gpt-3.5-turbo               0          0             9   \n",
       "f2266d87  gpt-3.5-turbo               0          0             4   \n",
       "eeace4bd  gpt-3.5-turbo               0          0             7   \n",
       "354fdce0  gpt-3.5-turbo               0          0             5   \n",
       "6eaa842f  gpt-3.5-turbo               0          0             5   \n",
       "a36a04d5  gpt-3.5-turbo               0          0             6   \n",
       "c9d5567f  gpt-3.5-turbo               0          0            10   \n",
       "c2e2ead1  gpt-3.5-turbo               0          0            10   \n",
       "9408e5f4  gpt-3.5-turbo               0          0             6   \n",
       "ee18417a  gpt-3.5-turbo               0          0             9   \n",
       "35f1f42a  gpt-3.5-turbo               0          0             6   \n",
       "55652348  gpt-3.5-turbo               0          0             5   \n",
       "72a3909f  gpt-3.5-turbo               0          0            11   \n",
       "\n",
       "          n_hyperbolic  n_abn_symbols  n_prompt  \n",
       "id                                               \n",
       "d429f032             2              0         0  \n",
       "1ce279be             4              0         0  \n",
       "c9595213             2              0         0  \n",
       "f2266d87             0              0         0  \n",
       "eeace4bd             4              0         0  \n",
       "354fdce0             3              0         0  \n",
       "6eaa842f             2              0         0  \n",
       "a36a04d5             4              0         0  \n",
       "c9d5567f             3              0         0  \n",
       "c2e2ead1             3              0         0  \n",
       "9408e5f4             5              0         0  \n",
       "ee18417a             5              0         1  \n",
       "35f1f42a             5              0         0  \n",
       "55652348             6              0         0  \n",
       "72a3909f             3              0         0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop(['prompt_id', 'kaggle_repo'], axis=1)\n",
    "train.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad85c490-c1e1-4c5c-b1fb-cad421a7b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important for pre-processing -- define which of the predictors are our featured engineered ones since we are using tf-idf later\n",
    "\n",
    "our_features = [\n",
    "   # \"misspelling_ratio\", \"n_misspellings\", \n",
    "    \"n_exclamations\", \"n_em_dash\", \"n_transition\", \"n_hyperbolic\",\"n_abn_symbols\", \"n_prompt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b4f352",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Perform all necessary EDA here. Before submitting, only keep those you feel are relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98bfb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_essay\n",
    "count_features = [\n",
    "    'n_misspellings',\n",
    "    'n_exclamations',\n",
    "    'n_em_dash',\n",
    "    'n_abn_symbols',\n",
    "    'n_transition_words',\n",
    "    'n_hyperbolic'\n",
    "]\n",
    "\n",
    "ratio_features = ['misspelling_ratio']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab85658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transition word plot\n",
    "plt.title('n_transition_words distribution by label') \n",
    "plt.xlabel('n_transition_words')\n",
    "plt.ylabel ('Density')\n",
    "sns.kdeplot(\n",
    "    data=df,\n",
    "    x='n_transition_words', \n",
    "    hue= 'generated', \n",
    "    common_norm=False, \n",
    "    fill=True, \n",
    "    alpha=0.4,\n",
    "    bw_adjust = 0.6\n",
    ")\n",
    "plt. show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperbolic plot\n",
    "sns.boxplot(data=df, x='generated', y='n_hyperbolic')\n",
    "plt.title(f'{'n_hyperbolic'} by label')\n",
    "plt.xlabel('authorship (0 = student, 1 = LLM generated)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecc260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stripplot showing abnormal symbols\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.stripplot(\n",
    "    data=df,\n",
    "    x='generated',\n",
    "    y='n_abn_symbols',\n",
    "    jitter=True,\n",
    "    alpha=0.3\n",
    ")\n",
    "plt.title(\"n_abn_symbols by Label\")\n",
    "plt.xlabel('authorship (0 = student, 1 = LLM generated)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f73a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation heatmap between features/target\n",
    "feature_cols = count_features + ratio_features + ['prompt_indicator']\n",
    "\n",
    "corr = df[feature_cols + ['generated']].corr()\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Among Engineered Features and Target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca4063",
   "metadata": {},
   "source": [
    "## 4. Data preparation pipelines and pre-processing\n",
    "\n",
    "Run all preparation and pre-processing pipelines here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27849082-8a20-4c9b-94fe-fa7cffb039c1",
   "metadata": {},
   "source": [
    "#### Splitting with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a04931d2-6631-4bf0-8994-b8ddabedf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train/test split\n",
    "train_essay, test_essay = train_test_split(train, test_size=0.3, random_state=38)\n",
    "\n",
    "# train\n",
    "train_y = train_essay['generated']\n",
    "train_X = train_essay.drop(columns='generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb9b75e9-2d17-4d20-9eef-048b44981b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "word_tfidf = TfidfVectorizer(stop_words='english', max_features=20000, ngram_range=(1,3), min_df=2)\n",
    "\n",
    "char_tfidf = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=20000, min_df=2)\n",
    "\n",
    "# combine the two vectorizer steps -- used Google AI overview for this and the next step, was not previously familiar \n",
    "# with Feature Union and making a more complex preprocessor\n",
    "nlp_features = FeatureUnion([\n",
    "    ('word', word_tfidf),\n",
    "    ('char', char_tfidf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49029497-37a4-4cc3-83eb-bd5273776df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing step -- used Google AI overview for this step, as described above\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('nlp', nlp_features, 'text'),\n",
    "        ('our', StandardScaler(), our_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('model', XGBClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db1c34",
   "metadata": {},
   "source": [
    "## 5. Model selection and hyperparameter tuning\n",
    "\n",
    "Perform all model selection and hyperpareter tuning here. Create separate pipelines here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420f051e-bb75-4ca6-b4b4-7e184527f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full grid search of all possible models\n",
    "### **important note*** this is not the exact code we used to run our comparison of models, this would've EXPLODED our computers!\n",
    "### we ran this model comparison grid search on preliminary training data (N=1000) and then as we advanced our sample size, \n",
    "### started to run grid search over independent models and their parameters individually, comparing AUC scores as our scoring \n",
    "### metric and evaluating performance on test data in kaggle. \n",
    "\n",
    "### this is formatted to represent all possible combos we tried, though it is not reflective of the multiple instances of \n",
    "### gridsearchcv we used\n",
    "\n",
    "\n",
    "# param_grid = [\n",
    "#    ENSEMBLE METHODS\n",
    "#    {\n",
    "#        \"model\": [RandomForestClassifier(random_state=38)],\n",
    "#        \"model__n_estimators\": [100, 200],\n",
    "#        \"model__max_depth\": [None, 5, 10],\n",
    "#        \"model__min_samples_split\": [5, 10, 20],\n",
    "#        \"model__class_weight\": [None, \"balanced\"]\n",
    "#    },\n",
    "#    {\n",
    "#        \"model\": [XGBClassifier(eval_metric='logloss', random_state=38)],\n",
    "#        \"model__learning_rate\": [0.05, 0.1],\n",
    "#        \"model__n_estimators\": [300, 500],\n",
    "#        \"model__max_depth\": [2, 3], \n",
    "#        \"model__reg_alpha\": [0, 0.1, 0.5],\n",
    "#        \"model__reg_lambda\": [3, 5]\n",
    "#    },\n",
    "#    LINEAR METHODS\n",
    "#    {\n",
    "#        \"model\": [LogisticRegression()],\n",
    "#        \"model__C\": [0.01, 0.05, 0.1, 0.5, 1, 2],\n",
    "#        \"model__penalty\": ['l1', 'l2'],\n",
    "#    }\n",
    "# ]\n",
    "\n",
    "# grid = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# grid.fit(train[['text'] + our_features], train['generated'])\n",
    "\n",
    "# print(\"Best score:\", grid.best_score_)\n",
    "# print(\"Best params:\", grid.best_params_)\n",
    "\n",
    "# pipe_final = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd0aedc",
   "metadata": {},
   "source": [
    "## 6. Creation of final pipeline\n",
    "\n",
    "Create your final pipeline here and save it in an object called `pipe_final`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('model', LogisticRegression(\n",
    "        max_iter=3000,\n",
    "        n_jobs=-1,\n",
    "        solver='saga'\n",
    "    ))\n",
    "])\n",
    "\n",
    "params_lr = {\n",
    "    'model__C': [5],\n",
    "    'model__penalty': ['l2'],\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(pipe_lr, params_lr, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "grid_lr.fit(train[['text'] + our_features], train['generated'])\n",
    "\n",
    "print(\"Logistic Reg best score:\", grid_lr.best_score_)\n",
    "print(\"Logistic Reg best params:\", grid_lr.best_params_)\n",
    "\n",
    "pipe_final = grid_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7bdb16-be96-4332-8a33-1aec39ab19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_preds = pipe_final.predict(test_X)\n",
    "test_probs = pipe_final.predict_proba(test_X)[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc = roc_auc_score(test_y, test_probs)\n",
    "print(\"Test ROC-AUC:\", roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac81cc",
   "metadata": {},
   "source": [
    "## 7. Creating Submission\n",
    "\n",
    "* Using `pipe_final`, create a data frame `final_submission` that has your predictions and write to `final_submission.csv` that you can submit on Kaggle. Note the format of `final_submission.csv` has to match that `example_submission.csv` exactly.\n",
    "* Take a screen shot of your final leaderboard score and ensure it displays below\n",
    "* Run the `RepeatedKFold()` as you did in PS3. \n",
    "\n",
    "\n",
    "* Keep track of at least one score a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918eb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pipe_final to create final_submission data frame here:\n",
    "preds = pipe_final.predict(train_essay_X)\n",
    "\n",
    "final_submission = pd.DataFrame({\n",
    "    'essay_id': train_essay.index,  # Use the same index as in your train_essay DataFrame\n",
    "    'generated': preds\n",
    "})\n",
    "\n",
    "final_submission.to_csv('data/final_submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ab19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/leaderboard.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0539526",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_final = RepeatedKFold(n_splits=5, n_repeats=10, random_state=38)\n",
    "scores_final = cross_val_score(pipe_final, X_train, y_train, scoring='accuracy', cv=cv_final)\n",
    "print(f'Mean accuracy: {np.mean(scores_final):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb5a36",
   "metadata": {},
   "source": [
    "## 8. Appendix\n",
    "\n",
    "Please anything extra that you don't feel is central, but would still like to keep here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987059b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
